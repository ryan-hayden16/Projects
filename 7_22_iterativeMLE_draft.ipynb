{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iterativeMLE_draft.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyff1DrRD9Ys/PosKFyhSy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryan-hayden16/Projects/blob/main/7_22_iterativeMLE_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preliminary steps"
      ],
      "metadata": {
        "id": "ACb79dw-HJBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports and installs\n",
        "!pip install scanpy # tools for scRNA-seq analysis\n",
        "!pip install matplotlib==3.1.3 # current version produces error w/ scanpy\n",
        "!pip install sklearn # tools for general data analysis\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scanpy as sc\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "!pip install matplotlib==3.1.3 # reinstall to force old package version"
      ],
      "metadata": {
        "id": "T5mtH5zxGoPv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5fa80fa4-fbc4-483c-dfa5-361f3e5d033b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scanpy\n",
            "  Downloading scanpy-1.9.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.10.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.11.2)\n",
            "Requirement already satisfied: importlib_metadata>=0.7 in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.12.0)\n",
            "Collecting anndata>=0.7.4\n",
            "  Downloading anndata-0.8.0-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=3 in /usr/local/lib/python3.7/dist-packages (from scanpy) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scanpy) (21.3)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from scanpy) (5.5.0)\n",
            "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.51.2)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.0.2)\n",
            "Collecting umap-learn>=0.3.10\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting session-info\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from scanpy) (1.1.0)\n",
            "Collecting matplotlib>=3.4\n",
            "  Downloading matplotlib-3.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from scanpy) (0.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from scanpy) (4.64.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from scanpy) (2.6.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.4->scanpy) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=3->scanpy) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy) (3.8.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (0.11.0)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)\n",
            "\u001b[K     |████████████████████████████████| 944 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->scanpy) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->scanpy) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib>=3.4->scanpy) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 55.8 MB/s \n",
            "\u001b[?25hCollecting stdlib_list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: umap-learn, pynndescent, session-info\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=d9daa7f7790faad4af42806f25587d8634b135ca17569f8fa2f04a8ae509a9d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=b757eefc4b33a381818514e27782b97365b6f8b2329f7cbef2ebc446d30199e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8048 sha256=abf09035e129971c2f997d109aa49ed296ce8e8f55809f7ee1a1f05d749870ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/ad/14/6a42359351a18337a8683854cfbba99dd782271f2d1767f87f\n",
            "Successfully built umap-learn pynndescent session-info\n",
            "Installing collected packages: fonttools, stdlib-list, pynndescent, matplotlib, umap-learn, session-info, anndata, scanpy\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed anndata-0.8.0 fonttools-4.34.4 matplotlib-3.5.2 pynndescent-0.5.7 scanpy-1.9.1 session-info-1.0.0 stdlib-list-0.8.0 umap-learn-0.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==3.1.3\n",
            "  Downloading matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1 MB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.1.3) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.5.2\n",
            "    Uninstalling matplotlib-3.5.2:\n",
            "      Successfully uninstalled matplotlib-3.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scanpy 1.9.1 requires matplotlib>=3.4, but you have matplotlib 3.1.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed matplotlib-3.1.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib==3.1.3 in /usr/local/lib/python3.7/dist-packages (3.1.3)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.1.3) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data (and metadata, if available)"
      ],
      "metadata": {
        "id": "iTTW7u9ajXjl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uTVsT3SBzgq"
      },
      "outputs": [],
      "source": [
        "# load data matrix (single patient/sample)\n",
        "x = pd.read_csv(\"/content/Kidney-counts.csv\", index_col=0)\n",
        "x = np.transpose(x) # transpose into cell by gene format"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load metadata (if available, can be used to check cluster accuracy)\n",
        "metadata = pd.read_csv(\"/content/annotations_FACS.csv\", index_col=0)\n",
        "metadata = metadata.loc[metadata['tissue'].isin(['Kidney'])]"
      ],
      "metadata": {
        "id": "6z6KfDLIGdjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove raw data with missing labels\n",
        "cellclass=metadata.cell_ontology_class\n",
        "cellclass=cellclass.to_frame()\n",
        "mergedf=x.merge(cellclass, left_index=True, right_index=True)\n",
        "metadf=mergedf.cell_ontology_class\n",
        "metadf=metadf.to_frame()\n",
        "bladf=mergedf.drop(columns=['cell_ontology_class'])\n",
        "\n",
        "# now raw data and metadata have matching sizes\n",
        "x=bladf\n",
        "x_labels=metadf\n",
        "\n",
        "# create annotated data matrix (ie: anndata) to use with scanpy\n",
        "adata_raw = sc.AnnData(X = x, obs = x_labels)"
      ],
      "metadata": {
        "id": "-wCkOuJ8KAN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46bfddee-c863-493d-f644-794962f57e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: FutureWarning: X.dtype being converted to np.float32 from int64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quality control of raw data (optional)"
      ],
      "metadata": {
        "id": "loEAml3ujeA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# quality control of raw data? (need old matplotlib version to avoid errors)\n",
        "\n",
        "# quality control\n",
        "adata_qc=adata_raw # keep copy of the raw data\n",
        "is_spike_in = {}\n",
        "for gene_name in adata_qc.var_names:\n",
        "    if 'ERCC' in gene_name:\n",
        "        is_spike_in[gene_name] = True # record that we found a spike-in\n",
        "    else:\n",
        "        is_spike_in[gene_name] = False # record that this was not a spike-in\n",
        "adata_qc.var['ERCC'] = pd.Series(is_spike_in) # label the spike ins\n",
        "qc = sc.pp.calculate_qc_metrics(adata_qc, qc_vars = ['ERCC']) # scanpy function\n",
        "cell_qc_dataframe = qc[0] # cell quality control\n",
        "gene_qc_dataframe = qc[1] # gene quality control\n",
        "\n",
        "# cell filtering and gene filtering\n",
        "low_ERCC_mask = (cell_qc_dataframe['pct_counts_ERCC'] < 10)\n",
        "adata_qc = adata_qc[low_ERCC_mask]\n",
        "sc.pp.filter_cells(adata_qc, min_genes = 750) # filter cells \n",
        "sc.pp.filter_genes(adata_qc, min_cells = 2) # filter genes\n",
        "sc.pp.filter_genes(adata_qc, min_counts = 10)\n",
        "\n",
        "#run PCA with no labels\n",
        "sc.pp.pca(adata_qc)\n",
        "sc.pl.pca_overview(adata_qc) # plot\n",
        "\n",
        "# run PCA as exploratory measure to check the data out\n",
        "sc.pp.pca(adata_qc)\n",
        "sc.pl.pca_overview(adata_qc, color='cell_ontology_class') # plot\n",
        "\n",
        "# normalize the data \n",
        "adata_norm=adata_qc # keep copy of qc data\n",
        "sc.pp.normalize_per_cell(adata_norm, counts_per_cell_after=1e6)\n",
        "sc.pp.normalize_total(adata_norm, target_sum=1e6, exclude_highly_expressed=True)\n",
        "\n",
        "# (OPTIONAL) Remove highly expressed genes distorting the data\n",
        "not_Rn45s = adata_norm.var.index != 'Rn45s'\n",
        "adata_no_Rn45s = adata_norm[:, not_Rn45s] # keep copy of normed data\n",
        "# need to check which genes to remove\n",
        "\n",
        "# scale the data\n",
        "adata_scale=adata_no_Rn45s\n",
        "# adata_scale=adata_norm\n",
        "sc.pp.log1p(adata_scale)\n",
        "sc.pp.scale(adata_scale)\n",
        "\n",
        "#re-run PCA with no labels\n",
        "sc.pp.pca(adata_scale)\n",
        "sc.pl.pca_overview(adata_scale) # plot\n",
        "\n",
        "# re-run PCA, should seperate data better this time\n",
        "sc.pp.pca(adata_scale)\n",
        "sc.pl.pca_overview(adata_scale, color='cell_ontology_class') # plot\n",
        "\n",
        "adata=adata_scale # adata is now quality controlled, normalized, and scaled"
      ],
      "metadata": {
        "id": "fq_NGXbEOMJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract count matrix from raw data"
      ],
      "metadata": {
        "id": "EcPf1JCnjp82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert data matrix in adata to dataframe\n",
        "x = pd.DataFrame(adata.X)\n",
        "x=x.set_index(adata.obs.index)\n",
        "\n",
        "#NOTE, numpy ndarray is preferable to dataframe because it can be more than 2 dimensions, but tensorflow tensor might be computationally advantageous"
      ],
      "metadata": {
        "id": "irKCY-vZQDr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Define known variables"
      ],
      "metadata": {
        "id": "490RutdYSQMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define known values\n",
        "C,G = x.shape # retrieve number of cells and genes from raw data matrix\n",
        "K = 5 # predicted number of cell-types\n",
        "L = 3 # predicted number of gene-communities (ie: high, medium, low expression communities)\n",
        "\n",
        "\n",
        "# define distribution f (start with Poisson) \n",
        "poisson pdf"
      ],
      "metadata": {
        "id": "Bt2cKQqeSX7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "python notes"
      ],
      "metadata": {
        "id": "iDlpmkIqcgui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create tensor in python\n",
        "import random\n",
        "C=50\n",
        "G=100\n",
        "K=5 \n",
        "L=3 \n",
        "Q=np.zeros((G,K,L))\n",
        "for i in range(G):\n",
        "  for j in range(K):\n",
        "    for l in range(L):\n",
        "      Q[i,j,l]=random.uniform(0, 1)\n",
        "\n",
        "print(Q)\n",
        "# Q is GxKxL tensor"
      ],
      "metadata": {
        "id": "q_6V-Mn4npNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate fake (normalized) count data\n",
        "X=np.zeros((C,G))\n",
        "for c in range(C):\n",
        "  for g in range(G):\n",
        "    X[c,g]=random.uniform(0,1)"
      ],
      "metadata": {
        "id": "pdFMKUiFsDJ9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X # maybe multiply each entry * 100, to avoid numerical errors????"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX589sUcSuje",
        "outputId": "54683bdf-57e1-4719-ddad-b10aafa3b7e5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.21326413, 0.61979383, 0.23554814, ..., 0.37794462, 0.35046691,\n",
              "        0.3547858 ],\n",
              "       [0.92062858, 0.47571902, 0.3754333 , ..., 0.30913323, 0.63323187,\n",
              "        0.99553775],\n",
              "       [0.29627568, 0.09308347, 0.04824277, ..., 0.4236403 , 0.81682611,\n",
              "        0.67860941],\n",
              "       ...,\n",
              "       [0.45548368, 0.44814593, 0.6696283 , ..., 0.40853438, 0.05663819,\n",
              "        0.22891408],\n",
              "       [0.70546348, 0.47833187, 0.54035059, ..., 0.51452846, 0.68232088,\n",
              "        0.69154152],\n",
              "       [0.1937172 , 0.36486966, 0.72162352, ..., 0.4815606 , 0.58059982,\n",
              "        0.88703628]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(threshold=np.inf)\n",
        "# force python to print entire array"
      ],
      "metadata": {
        "id": "w36x7jhO3rEW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize S and T variables"
      ],
      "metadata": {
        "id": "dhO_yZPhkG5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize S\n",
        "import scipy as sp\n",
        "from numpy import linalg as LA\n",
        "from scipy.linalg import sqrtm\n",
        "from numpy.linalg import inv\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "A=np.dot(X,np.transpose(X)) #step 1 (affinity matrix, simple version)\n",
        "\n",
        "D=np.zeros((C,C))\n",
        "for i in range(C):\n",
        "  D[i,i] = sum(A[i,:]) #step 2.a (graph laplacian)\n",
        "\n",
        "E=sqrtm(D) \n",
        "\n",
        "# note that np.dot(E,E)==D returns a few falses, possibly because of numerical error? should be identical\n",
        "\n",
        "F=inv(E) \n",
        "H=np.dot(F,np.dot(A,F)) #step 2.b (graph laplacian)\n",
        "\n",
        "# note that H==np.transpose(H) returns some falses, ie: not symmetric yet we get all real eigenvalues here\n",
        "\n",
        "w, v = LA.eig(H) #step 3 (find K largest (orthogonal) eigenvectors of H and form a CxK matrix with them) and normalize the matrix\n",
        "ordered_eigval=np.argsort(w) # returns indexes of ordered (small to large) of w (eigenvalue list)\n",
        "k_large_eigval = ordered_eigval[-K:] # returns indexes of K largest eigvals\n",
        "k_large_eigvec=np.transpose(v[k_large_eigval]) # returns corresponding K largest eigvecs, as a CxK ndarray\n",
        "Y = normalize(k_large_eigvec, axis=1, norm='l2') #normalize rows of the CxK matrix\n",
        "\n",
        "\n",
        "#step 5 (treat each of the C rows of the matrix as a K-dim vector and cluster into K-clusters, via K-means)\n",
        "kmeans = KMeans(n_clusters=K, random_state=0).fit(Y)\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "#step 6 (assign/label each of the C cells into the corresponding cluster (ie: labels are 1,2,...,K) from step 5)\n",
        "S=np.zeros((C,K))\n",
        "for i in range(C):\n",
        "  for j in range(K):\n",
        "    if cluster_labels[i]==j:\n",
        "      S[i,j]=1\n",
        "#(each cell now has a label from 1,...,K, so we can then form the CxK classification matrix (ie: matrix S) that we want)\n",
        "\n",
        "\n",
        "# this process gives us S_0\n",
        "\n",
        "# THIS CODE IS FINISHED AND HAS BEEN CHECKED TO BE WORKING PROPERLY"
      ],
      "metadata": {
        "id": "FyVuZx30S9o1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_labels"
      ],
      "metadata": {
        "id": "JBhrFbbc70gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xweJDcYG7wgt",
        "outputId": "853d3d96-5158-4dfb-d522-16cb74f42bb0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize T\n",
        "\n",
        "T=np.zeros((G,K,L))\n",
        "\n",
        "for i in range(K):\n",
        "  XT=X\n",
        "  XT=np.delete(XT,np.where(cluster_labels!=i),0) #now XT should only contain rows with label i\n",
        "  A=np.dot(np.transpose(XT),XT) #step 1 (affinity matrix, simple version)\n",
        "  D=np.zeros((G,G))\n",
        "  for l in range(G):\n",
        "    D[l,l] = sum(A[:,l]) #step 2.a (graph laplacian)\n",
        "  E=sqrtm(D) \n",
        "  F=inv(E) \n",
        "  H=np.dot(F,np.dot(A,F)) \n",
        "\n",
        "# H should be symmetric, so shouldnt even have complex eigenvectors\n",
        "\n",
        "  w, v = LA.eig(H) #step 3 (find K largest (orthogonal) eigenvectors of H and form a GxK matrix with them) and normalize the matrix\n",
        "  ordered_eigval=np.argsort(w) # returns indexes of ordered (small to large) of w (eigenvalue list)\n",
        "  k_large_eigval = ordered_eigval[-K:] # returns indexes of K largest eigvals\n",
        "  k_large_eigvec=np.transpose(v[k_large_eigval]) # returns corresponding K largest eigvecs, as a GxK ndarray\n",
        "  complex_k_large_eigvec=np.array(k_large_eigvec, dtype = 'complex_') # convert to complex valued matrix? does not fix error\n",
        "  e=LA.norm(complex_k_large_eigvec, axis=1) #\n",
        "  for n in range(C):\n",
        "    complex_k_large_eigvec[n,:]=complex_k_large_eigvec[n,:]/e[n]\n",
        "  Y=complex_k_large_eigvec\n",
        "  B=Y.real # possible solution: just consider real part (do this in S initialization as well)\n",
        "  # whatever solution is, apply to S initialization as well in case its eigenvalues are complex\n",
        "  #try dividing each vector in k_large_eigvec by the corresponding value in LA.norm(k_large_eigvec), ie manually scale to unit vector\n",
        "  # Y = normalize(complex_k_large_eigvec, axis=1, norm='l2') #normalize rows of the GxK matrix (CAUSES ERROR DUE TO COMPLEX NUMBERS)\n",
        "  #step 5 (treat each of the C rows of the matrix as a K-dim vector and cluster into K-clusters, via K-means)\n",
        "  #above solution seemed to work, but no error is in k-means\n",
        "\n",
        "# ASK YUNPENG HOW TO DEAL WITH K-MEANS IF VECTORS ARE COMPLEX VALUED ??? MAYBE JUST DROP IMAGINARY PART? OR TAKE MAGNITUDE? ITS ONLY AN INITIAL GUESS FOR S\n",
        "\n",
        "  kmeans = KMeans(n_clusters=L, random_state=0).fit(B)\n",
        "  Tcluster_labels = kmeans.labels_\n",
        "  #step 6 (assign/label each of the C cells into the corresponding cluster (ie: labels are 1,2,...,K) from step 5)\n",
        "  for g in range(G):\n",
        "    for l in range(L):\n",
        "      if Tcluster_labels[g]==l:\n",
        "        T[g,i,l]=1\n",
        "\n",
        "# ALSO MAKE SURE TO SWAP TRANSPOSE ORDER SO ITS NOW GxG AFFINITY MATRIX, AND SWAP WHEREVER ELSE NECCESARY, IE NOW K-MEANS ON COLUMNS AND NORMALIZE COLUMNS, ETC...\n"
      ],
      "metadata": {
        "id": "Amxm1NYR-Q9T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T"
      ],
      "metadata": {
        "id": "XgVjn03OZSOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdVq-UrPJxjq",
        "outputId": "0de8b63f-240c-4ba2-f945-dadd63048802"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now code seems to work, but clean it up, make sure no logic errors, and change S initialization code so its also able to deal with complex #'s"
      ],
      "metadata": {
        "id": "cdB9G4fdZXAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "notes/errors"
      ],
      "metadata": {
        "id": "xhFYpe2Hvjp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "e=LA.norm(k_large_eigvec, axis=1)\n",
        "for i in range(C):\n",
        "  k_large_eigvec[i,:]=k_large_eigvec[i,:]/e[i]\n",
        "#this can replace sklearn l2 normalizer, check below for accuracy"
      ],
      "metadata": {
        "id": "lbCVBgPcQe7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XT=X\n",
        "XT=np.delete(XT,np.where(cluster_labels!=0),0) #now XT should only contain rows with label i\n",
        "A=np.dot(np.transpose(XT), XT) #step 1 (affinity matrix, simple version)\n",
        "D=np.zeros((G,G))\n",
        "for g in range(G):\n",
        "  D[g,g] = sum(A[:,g]) #step 2.a (graph laplacian)\n",
        "  # maybe change g to column instead of row\n",
        "E=sqrtm(D) \n",
        "F=inv(E) \n",
        "H=np.dot(F,np.dot(A,F)) \n",
        "w, v = LA.eig(H)"
      ],
      "metadata": {
        "id": "RBCCnDhe-J8W"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize theta paramaters"
      ],
      "metadata": {
        "id": "-AO8mGGOcwDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu=np.zeros((K,L))\n",
        "for k in range(K):\n",
        "  for l in range(L):\n",
        "    numer=0\n",
        "    denom=0\n",
        "    for c in range(C):\n",
        "      for g in range(G):\n",
        "        numer=numer+(S[c,k]*T[g,k,l]*X[c,g])\n",
        "        denom=denom+(S[c,k]*T[g,k,l])\n",
        "    mu[k,l]=numer/denom"
      ],
      "metadata": {
        "id": "TjycJjPlWN9U"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numer=0\n",
        "denom=0\n",
        "  for c in range(C):\n",
        "    for g in range(G):\n",
        "      numer=numer+(S[c,k]*T[g,k,l]*X[c,g])\n",
        "      denom=denom+(S[c,k]*T[g,k,l])"
      ],
      "metadata": {
        "id": "-sDvaEFAWUWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DScRqceWWPRs",
        "outputId": "4694b75a-2726-4e77-f4d3-2e91ae172fc4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.49497398, 0.50071955, 0.50456214],\n",
              "       [0.48855569, 0.49526875, 0.5103612 ],\n",
              "       [0.5068372 , 0.49999404, 0.46634825],\n",
              "       [0.52030205, 0.49395987, 0.50169942],\n",
              "       [0.51902416, 0.48830684, 0.50092061]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using S_0 and T_0 and X, we can directly calculate rho_0, pi_0, and mu_0\n",
        "\n",
        "#rho: use yunpeng line (12) and S_0\n",
        "rho=np.zeros(K)\n",
        "for k in range(K):\n",
        "  rho[k]=(1/C)*sum(S[:,k])\n",
        "\n",
        "#pi: use yunpeng line (13) and T_0\n",
        "pi=np.zeros((K,L))\n",
        "for k in range(K):\n",
        "  for l in range(L):\n",
        "    pi[k,l]=(1/G)*sum(T[:,k,l])\n",
        "\n",
        "\n",
        "#mu: use yunpeng line (14) and S_0, T_0, X \n",
        "mu=np.zeros((K,L))\n",
        "for k in range(K):\n",
        "  for l in range(L):\n",
        "    numer=0\n",
        "    denom=0\n",
        "    for c in range(C):\n",
        "      for g in range(G):\n",
        "        numer=numer+(S[c,k]*T[g,k,l]*X[c,g])\n",
        "        denom=denom+(S[c,k]*T[g,k,l])\n",
        "    mu[k,l]=numer/denom\n",
        "\n",
        "# double check logic of mu loop"
      ],
      "metadata": {
        "id": "50X4L91Ucyk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterative maximization scheme"
      ],
      "metadata": {
        "id": "ITNRNSiPcdH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EM steps\n",
        "\n",
        "S_old = 0\n",
        "S_new = S\n",
        "\n",
        "T_old = 0\n",
        "T_new = T\n",
        "\n",
        "epsilon_1 = .01\n",
        "epsilon_2 + .01\n",
        "\n",
        "while ((S_new - S_old > epsilon_1) && (T_new - T_old > epsilon_2)) #something like this, maybe also check theta convergence???\n",
        "# note that S and T are matrices so cant just subtract, need to take norm or something, \n",
        "# ask yunpeng best way to measure S and T convergence\n",
        "\n",
        "# update S, rho, mu\n",
        "\n",
        "S_old=S_new # save copy of previous S update\n",
        "for c in range(C):\n",
        "  for k in range(K):\n",
        "    S_new[c,k]= # update S, using updated rho, mu, and T values\n",
        "    # S_NEW IS LINE (9)\n",
        "\n",
        "for k in range(K):\n",
        "  rho[k]=(1/C)*sum(S_new[:,k]) # update rho, using updated S values\n",
        "\n",
        "for k in range(K):\n",
        "  for l in range(L):\n",
        "    numer=0\n",
        "    denom=0\n",
        "    for c in range(C):\n",
        "      for g in range(G):\n",
        "        numer=numer+(S_new[c,k]*T_new[g,k,l]*X[c,g])\n",
        "        denom=denom+(S_new[c,k]*T_new[g,k,l])\n",
        "    mu[k,l]=numer/denom # update mu, using updated S and T values\n",
        "\n",
        "\n",
        "# update T, pi, mu\n",
        "\n",
        "T_old=T_new # save copy of previous S update\n",
        "for g in range(G):\n",
        "  for k in range(K):\n",
        "    for l in range(L):\n",
        "      T_new[g,k,l]= # update T, using updated pi, mu, and S values\n",
        "      # T_NEW IS LINE (10)\n",
        "\n",
        "for k in range(K):\n",
        "  for l in range(L):\n",
        "    pi[k,l]=(1/G)*sum(T_new[:,k,l]) # update pi, using updated T values\n",
        "\n",
        "for k in range(K):\n",
        "  for l in range(L):\n",
        "    numer=0\n",
        "    denom=0\n",
        "    for c in range(C):\n",
        "      for g in range(G):\n",
        "        numer=numer+(S_new[c,k]*T_new[g,k,l]*X[c,g])\n",
        "        denom=denom+(S_new[c,k]*T_new[g,k,l])\n",
        "    mu[k,l]=numer/denom # update mu, using updated S and T values"
      ],
      "metadata": {
        "id": "Qw4B9l8qWuO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start with S_0, T_0, rho_0, pi_0, mu_0\n",
        "\n",
        "# loop through steps 1 and 2\n",
        "\n",
        "\n",
        "#NOTE: be careful with regard to dropouts/blowups, use appropriate numerical techniques\n",
        "\n",
        "\n",
        "# step 1: update S, rho, mu\n",
        "fix previous values of S, T, rho, pi, mu \n",
        "calculate updated S # E step (define function) \n",
        "USE YUNPENG LINE (9)\n",
        "calculate updated rho, mu # M step (define function)\n",
        "USE YUNPENG LINES (12 & 14)\n",
        "\n",
        "# step 2: update T, pi, mu\n",
        "fix previous values of S, T, rho, pi, mu \n",
        "calculate updated T # E step (define function)\n",
        "USE YUNPENG LINE (10)\n",
        "calculate updated pi, mu # M step (define function)\n",
        "USE YUNPENG LINES (13 & 14)\n",
        "\n",
        "# stop loop if convergence criteria met, ie: S and T both no longer changing values to significant degree\n",
        "IE: T_(M+1)-T_(M) < epsilon, and similar for S_(M)"
      ],
      "metadata": {
        "id": "x0NqfJYfZ2AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cXlixFjkVwiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert S and T into labels Z and W\n",
        "# check Z (cell-type) labels against true labels from metadata\n",
        "# check high expression gene-community against known biomarkers for associated cell-type"
      ],
      "metadata": {
        "id": "JHWsWooVaiIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# figure out way to visualize Z and W labels"
      ],
      "metadata": {
        "id": "X_Z26Zcqa5fl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}